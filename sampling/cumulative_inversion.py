import matplotlib.pyplot as plt
import numpy as np
import torch

# --- 1. Define Distribution and Grid Parameters ---
mean = 0.0
std = 0.5
n_samples = 5000  # The total number of random samples we want to generate.
n_points = 10000  # The number of points to use for our discrete approximation of the distribution.
                  # A higher number results in a more accurate CDF and better samples.
x_range = [-5, 5] # The range of x-values to consider. This should be wide enough
                  # to capture the vast majority of the probability mass of the distribution.

# --- 2. Define the Sampling Function ---
def cum_inv(mean, std, n_points, x_range, n_samples):
    """
    Generates samples from a normal distribution using the inverse transform sampling method.
    """
    # --- Step A: Pre-calculate the Cumulative Distribution Function (CDF) ---

    # Create a PyTorch Normal distribution object with the given mean and std.
    norm = torch.distributions.Normal(mean, std)

    # Create a 1D grid of evenly spaced points (our x-axis).
    values = torch.linspace(x_range[0], x_range[1], n_points)

    # Calculate the Probability Density Function (PDF), p(x), for each point on the grid.
    pdf = torch.exp(norm.log_prob(values))

    # Calculate the step size 'dx' between points on our grid. This is needed for the integral.
    dx = values[1] - values[0]

    # Approximate the CDF by numerically integrating the PDF.
    # We do this by taking the cumulative sum of the PDF values and multiplying by dx.
    cdf = torch.cumsum(pdf * dx, dim=0)

    # Normalize the CDF to ensure its maximum value is exactly 1.0. This corrects for
    # any minor numerical errors or mass outside our defined x_range.
    cdf = cdf / cdf[-1]

    # --- Step B: Generate Samples using the Inverse CDF ---

    # Generate `n_samples` random numbers from a Uniform(0, 1) distribution.
    # These are the 'y-values' on our CDF that we will invert.
    u_samples = torch.rand(n_samples)

    # This is the core of the "inversion". `torch.searchsorted` finds the index
    # where each `u_sample` would fit into the sorted `cdf` array. This is a
    # very fast, vectorized way to find the corresponding x-value for each u.
    sample_indices = torch.searchsorted(cdf, u_samples)

    # The final samples are the x-values from our original grid at the found indices.
    return values[sample_indices]

# --- 3. Generate Samples and Plot for Verification ---

# Call our function to generate samples using the inverse transform method.
generated_samples = cum_inv(mean, std, n_points, x_range, n_samples)

# For comparison, let's also generate samples using PyTorch's highly optimized, built-in sampler.
# This gives us a "ground truth" to compare our results against.
norm = torch.distributions.Normal(mean, std)
true_samples = norm.sample((n_samples,))

plt.figure(figsize=(12, 6))

# --- Plotting the Histograms ---
# A histogram is a visual representation of the distribution of a dataset.
# It groups the data into 'bins' and plots the number of data points in each bin.

# `density=True`: This is a crucial parameter. It normalizes the histogram so that
# the total area of all the bars equals 1. This allows us to directly compare
# the shape of the histogram to a true probability density function (PDF).

# Plot a histogram of the samples generated by our algorithm.
plt.hist(generated_samples.numpy(), bins=50, density=True, alpha=0.7, label='Our Samples (Inverse CDF)')

# On the same plot, overlay a histogram of the "true" samples from PyTorch.
# If our algorithm is correct, the two histograms should look nearly identical.
plt.hist(true_samples.numpy(), bins=50, density=True, alpha=0.7, label='True Samples (PyTorch)')

plt.title(f"Sampling from N({mean}, {std}) via Inverse Transform")
plt.xlabel("Value")
plt.ylabel("Density")
plt.legend()
plt.grid(True)
plt.show()
